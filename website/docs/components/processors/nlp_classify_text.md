---
title: nlp_classify_text
slug: nlp_classify_text
type: processor
status: beta
categories: ["Machine Learning","NLP"]
---

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the corresponding source file under internal/impl/<provider>.
-->

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

:::caution BETA
This component is mostly stable but breaking changes could still be made outside of major version releases if a fundamental problem with the component is found.
:::
Performs text classification using a Hugging Face ðŸ¤— NLP pipeline with an ONNX Runtime model.

Introduced in version v1.9.0.


<Tabs defaultValue="common" values={[
  { label: 'Common', value: 'common', },
  { label: 'Advanced', value: 'advanced', },
]}>

<TabItem value="common">

```yml
# Common config fields, showing default values
label: ""
nlp_classify_text:
  pipeline_name: "" # No default (optional)
  model_path: /model_repository
  model_download_options: {}
  aggregation_function: SOFTMAX
  multi_label: false
```

</TabItem>
<TabItem value="advanced">

```yml
# All config fields, showing default values
label: ""
nlp_classify_text:
  pipeline_name: "" # No default (optional)
  model_path: /model_repository
  onnx_library_path: /usr/lib/onnxruntime.so
  onnx_filename: ""
  enable_model_download: false
  model_download_options:
    model_repository: ""
  aggregation_function: SOFTMAX
  multi_label: false
```

</TabItem>
</Tabs>

### Text Classification
Text Classification is the task of assigning a label or class to a given text.Some use cases are sentiment analysis, natural language inference, and assessing grammatical correctness.
This processor runs text-classification inference against batches of text data, returning labelled classification corresponding to each input.
This component uses [Hugot](https://github.com/knights-analytics/hugot), a library that provides an interface for running [Open Neural Network Exchange (ONNX) models](https://onnx.ai/onnx/intro/) and transformer pipelines, with a focus on NLP tasks.

Currently, [HuggingBento only implements](https://github.com/knights-analytics/hugot/tree/main?tab=readme-ov-file#implemented-pipelines):
	
- [featureExtraction](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.FeatureExtractionPipeline)
- [textClassification](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.TextClassificationPipeline)
- [tokenClassification](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.TokenClassificationPipeline)

### What is a pipeline?
From [HuggingFace docs](https://huggingface.co/docs/transformers/en/main_classes/pipelines):
> A pipeline in ðŸ¤— Transformers is an abstraction referring to a series of steps that are executed in a specific order to preprocess and transform data and return a prediction from a model. Some example stages found in a pipeline might be data preprocessing, feature extraction, and normalization.

:::warning
While, only models in [ONNX](https://onnx.ai/) format are supported, exporting existing formats to ONNX is both possible and straightforward in most standard ML libraries. For more on this, check out the [ONNX conversion docs](https://onnx.ai/onnx/intro/converters.html). 
Otherwise, check out using [HuggingFace Optimum](https://huggingface.co/docs/optimum/en/exporters/onnx/usage_guides/export_a_model) for easy model conversion.
:::


## Examples

<Tabs defaultValue="Emotion Scoring (Local Model)" values={[
{ label: 'Emotion Scoring (Local Model)', value: 'Emotion Scoring (Local Model)', },
{ label: 'Sentiment Analysis (Downloaded Model)', value: 'Sentiment Analysis (Downloaded Model)', },
]}>

<TabItem value="Emotion Scoring (Local Model)">

Here, we load the [Cohee/distilbert-base-uncased-go-emotions-onnx](https://huggingface.co/Cohee/distilbert-base-uncased-go-emotions-onnx) model from the local directory at `models/coheedistilbert_base_uncased_go_emotions_onnx`.The processor returns a single-label output with the highest emotion score for the text. 

```yaml
pipeline:
  processors:
    - nlp_classify_text:
        pipeline_name: classify-incoming-data
        model_path: "models/coheedistilbert_base_uncased_go_emotions_onnx"

# In: "I'm super excited for my Bento box!"
# Out: [{"Label":"excitement","Score":0.34134513}]
```

</TabItem>
<TabItem value="Sentiment Analysis (Downloaded Model)">

Here, we retrieve the [KnightsAnalytics/distilbert-base-uncased-finetuned-sst-2-english(https://huggingface.co/KnightsAnalytics/distilbert-base-uncased-finetuned-sst-2-english) model from HuggingFace and store it in a `./models` directory.The processor returns a multi-label output indicating showing a `POSITIVE` and `NEGATIVE` score some input text-data.

```yaml
pipeline:
  processors:
    - nlp_classify_text:
        pipeline_name: classify-multi-label
        multi_label: true
        model_path: "./models"
        enable_model_download: true
        model_download_options:
          model_repository: "KnightsAnalytics/distilbert-base-uncased-finetuned-sst-2-english"


# In: "This meal tastes like old boots."
# Out: [{"Label":"NEGATIVE","Score":0.9977291},{"Label":"POSITIVE","Score":0.0022708932}]
```

</TabItem>
</Tabs>

## Fields

### `pipeline_name`

Name of the pipeline. Defaults to uuid_v4() if not set


Type: `string`  

### `model_path`

Path to the ONNX model directory. If `enable_model_download` is `true`, the model will be downloaded here.


Type: `string`  
Default: `"/model_repository"`  

```yml
# Examples

model_path: /path/to/models/my_model.onnx
```

### `onnx_library_path`

The location of the ONNX Runtime dynamic library.


Type: `string`  
Default: `"/usr/lib/onnxruntime.so"`  

### `onnx_filename`

The filename of the model to run. Only necessary to specify when multiple .onnx files are present.


Type: `string`  
Default: `""`  

```yml
# Examples

onnx_filename: model.onnx
```

### `enable_model_download`

If enabled, attempts to download an ONNX Runtime compatible model from HuggingFace specified in `model_name`.


Type: `bool`  
Default: `false`  

### `model_download_options`

Sorry! This field is missing documentation.


Type: `object`  

### `model_download_options.model_repository`

The name of the huggingface model repository.


Type: `string`  
Default: `""`  

```yml
# Examples

model_repository: KnightsAnalytics/distilbert-NER

model_repository: KnightsAnalytics/distilbert-base-uncased-finetuned-sst-2-english

model_repository: sentence-transformers/all-MiniLM-L6-v2
```

### `aggregation_function`

The aggregation function to use for the text classification pipeline.


Type: `string`  
Default: `"SOFTMAX"`  
Options: `SOFTMAX`, `SIGMOID`.

### `multi_label`

Whether a text classification pipeline should return multiple labels. If false, only the label-pair with the highest score is returned.


Type: `bool`  
Default: `false`  


